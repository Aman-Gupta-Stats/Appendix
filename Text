from pyspark.sql import SparkSession

# Assuming SparkSession is already created
spark = SparkSession.builder.getOrCreate()

# Sample structure (replace these with your actual DataFrames)
# df1 = ...
# df2 = ...

# Step 1: Select distinct values from both columns (optional but faster if values repeat)
df1_distinct = df1.select("col1").distinct()
df2_distinct = df2.select("col2").distinct()

# Step 2: Perform an inner join on the two DataFrames
common_values_df = df1_distinct.join(df2_distinct, df1_distinct.col1 == df2_distinct.col2, "inner")

# Step 3: Count the number of common values
common_count = common_values_df.count()

print(f"Number of common values: {common_count}")




X_test_seq = pad_sequences(test_seq, maxlen=maxlen)

# Build LSTM model
model = Sequential()
model.add(Embedding(input_dim=10000, output_dim=128, input_length=maxlen))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Train
model.fit(X_train_seq, y_train, epochs=5, batch_size=32, validation_split=


model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=128))
model.add(Bidirectional(LSTM(128, return_sequences=True)))
model.add(Dropout(0.3))
model.add(BatchNormalization())

model.add(LSTM(64))
model.add(Dropout(0.3))
model.add(BatchNormalization())

model.add(Dense(32, activation=‘relu’))
model.add(Dropout(0.3))
model.add(Dense(1, activation=‘sigmoid’))





